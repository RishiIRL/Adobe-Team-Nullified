{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb8cdf11-a717-41d9-a101-2e1cfb0d2b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Current Working Directory: C:\\Users\\surya\\Desktop\\sample_project_1\\challenge_1b\n",
      "üîç Processing: collection 1\n",
      "‚úÖ Done: .\\collection 1\\challenge1b_output.json\n",
      "üîç Processing: collection 2\n",
      "‚úÖ Done: .\\collection 2\\challenge1b_output.json\n",
      "üîç Processing: collection 3\n",
      "‚úÖ Done: .\\collection 3\\challenge1b_output.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from utils.parse_pdf import extract_sections\n",
    "from utils.embedder import load_model, get_embedding\n",
    "from utils.ranker import rank_sections\n",
    "\n",
    "\n",
    "\n",
    "BASE_DIR = \".\"\n",
    "COLLECTION_PREFIX = \"collection\"\n",
    "OUTPUT_FILENAME = \"challenge1b_output.json\"\n",
    "INPUT_FILENAME = \"challenge1b_input.json\"\n",
    "PDF_DIR_NAME = \"PDFs\"\n",
    "\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "\n",
    "for entry in os.listdir(BASE_DIR):\n",
    "    collection_path = os.path.join(BASE_DIR, entry)\n",
    "    if not os.path.isdir(collection_path) or not entry.startswith(COLLECTION_PREFIX):\n",
    "        continue\n",
    "\n",
    "    input_json_path = os.path.join(collection_path, INPUT_FILENAME)\n",
    "    pdf_dir = os.path.join(collection_path, PDF_DIR_NAME)\n",
    "    output_json_path = os.path.join(collection_path, OUTPUT_FILENAME)\n",
    "\n",
    "    if not os.path.exists(input_json_path) or not os.path.isdir(pdf_dir):\n",
    "        print(f\"‚ö†Ô∏è Skipping {entry}: Missing input JSON or PDFs folder.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üîç Processing: {entry}\")\n",
    "\n",
    "    \n",
    "    with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    query = data[\"job_to_be_done\"][\"task\"]\n",
    "    documents = data[\"documents\"]\n",
    "    query_emb = get_embedding(model, query)\n",
    "\n",
    "    extracted_sections = []\n",
    "    subsection_analysis = []\n",
    "    input_filenames = [d[\"filename\"] for d in documents]\n",
    "\n",
    "    for rank, doc in enumerate(documents, start=1):\n",
    "        fname = doc[\"filename\"]\n",
    "        pdf_path = os.path.join(pdf_dir, fname)\n",
    "\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"üö´ Missing file: {pdf_path}\")\n",
    "            continue\n",
    "\n",
    "        sections, doc_title = extract_sections(pdf_path)\n",
    "\n",
    "        if doc_title:\n",
    "            section_title = doc_title\n",
    "            selected_page = 1\n",
    "            selected_text = \"\\n\".join([s[\"text\"] for s in sections if s[\"page\"] == 1])\n",
    "        else:\n",
    "            ranked = rank_sections(query_emb, sections, model)\n",
    "            if not ranked:\n",
    "                continue\n",
    "            top_sec = ranked[0]\n",
    "            section_title = top_sec.get(\"title\", top_sec[\"text\"][:40])\n",
    "            selected_page = top_sec[\"page\"]\n",
    "            selected_text = top_sec[\"text\"]\n",
    "\n",
    "        extracted_sections.append({\n",
    "            \"document\": fname,\n",
    "            \"section_title\": section_title,\n",
    "            \"importance_rank\": rank,\n",
    "            \"page_number\": selected_page\n",
    "        })\n",
    "\n",
    "        subsection_analysis.append({\n",
    "            \"document\": fname,\n",
    "            \"refined_text\": selected_text,\n",
    "            \"page_number\": selected_page\n",
    "        })\n",
    "\n",
    "    final_output = {\n",
    "        \"metadata\": {\n",
    "            \"input_documents\": input_filenames,\n",
    "            \"persona\": data.get(\"persona\", \"Unknown\"),\n",
    "            \"job_to_be_done\": query,\n",
    "            \"processing_timestamp\": datetime.datetime.now().isoformat()\n",
    "        },\n",
    "        \"extracted_sections\": extracted_sections,\n",
    "        \"subsection_analysis\": subsection_analysis\n",
    "    }\n",
    "\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Done: {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d6188-f35f-4080-8c52-3b1c8c727966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
